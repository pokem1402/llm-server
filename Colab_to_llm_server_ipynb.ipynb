{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoIxquAlJ2Wl"
      },
      "source": [
        "# GPU 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BWf_RA6J6pJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "assert torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZG1H6pdKRAG"
      },
      "outputs": [],
      "source": [
        "# Google Drive Mount\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzroHQLELZfN"
      },
      "source": [
        "# Install text-generation web ui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQmJmY2A8c-N"
      },
      "outputs": [],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "_ekDup0TstGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5Qi2pWMUWlb"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/oobabooga/text-generation-webui.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu3f5NQ2UbMf"
      },
      "outputs": [],
      "source": [
        "!pip install -r ./text-generation-webui/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRcyVEBWWHsP"
      },
      "source": [
        "# load Some file to colab local drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configure for your setting\n",
        "\n",
        "repo_name = \"davidkim205/komt-mistral-7b-v1\"\n",
        "backup_path = \"/content/drive/MyDrive/자료\"\n",
        "qtype = \"q8_0\"\n"
      ],
      "metadata": {
        "id": "KOuZTMyB1tvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDqd3PJcUO3o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "web_ui_path = Path(\"/content/text-generation-webui/\")\n",
        "model_file_name = repo_name.split(\"/\")[-1]+\"_\"+qtype+\".bin\"\n",
        "model_name = repo_name.split('/')[-1]\n",
        "model_file_path = Path(backup_path) / model_file_name\n",
        "dest_model_path = web_ui_path / \"models\" / model_file_name\n",
        "\n",
        "# if there is no model file in google drive, download, quantize, copy and paste.\n",
        "if not os.path.exists(model_file_path):\n",
        "  !git clone https://github.com/ggerganov/llama.cpp.git\n",
        "  %cd llama.cpp\n",
        "  !git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "  !pip install -r requirements.txt\n",
        "  %cd models\n",
        "  !git clone https://huggingface.co/{repo_name}.git\n",
        "  fp16 = f\"{model_name}.gguf.fp16.bin\"\n",
        "  %cd ..\n",
        "  !python convert.py models/{model_name} --outtype f16 --outfile models/{fp16} # convert to fp16\n",
        "  !rm -rf models/{model_name}\n",
        "  !./quantize models/{fp16} {dest_model_path} {qtype} # convert to q8\n",
        "  !cp {dest_model_path} {model_file_path}\n",
        "  !rm models/{fp16}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert os.path.exists(model_file_path), model_file_path\n",
        "if not os.path.exists(dest_model_path):\n",
        "  !cp {model_file_path} {dest_model_path}\n",
        "  print(\"model file has been loaded\")\n",
        "else:\n",
        "  print(\"model file was loaded already\")\n",
        "%cd {web_ui_path}"
      ],
      "metadata": {
        "id": "sJWi0w-DNu0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeaEAxTsdgVq"
      },
      "source": [
        "# LLM Local Model Configuration\n",
        "\n",
        "https://github.com/oobabooga/text-generation-webui/blob/main/settings-template.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFddJ13ddfTw"
      },
      "outputs": [],
      "source": [
        "custom_config = {\n",
        "    \"max_new_tokens\" : 1024,\n",
        "    \"truncation_length\" : 5120,\n",
        "    \"instruction_template\" : \"Mistral\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzKHxNYQci-R"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pprint import pprint\n",
        "\n",
        "config_template_file_name = \"settings-template.yaml\"\n",
        "config_template_path = web_ui_path / config_template_file_name\n",
        "\n",
        "with open(config_template_path, 'r') as f:\n",
        "  config  = yaml.safe_load(f)\n",
        "\n",
        "config = {**config, **custom_config}\n",
        "pprint(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbrSciEcmS4F"
      },
      "source": [
        "# API extension 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akQL1UI7mRjF"
      },
      "outputs": [],
      "source": [
        "!pip install -r ./extensions/api/requirements.txt\n",
        "!pip install -r ./extensions/openai/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JADU4zrmmP38"
      },
      "source": [
        "# Server 구동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boPHWaWv2GzW"
      },
      "outputs": [],
      "source": [
        "n_gpu_layers = 35\n",
        "n_ctx = 5120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KSNHp2HbOCT"
      },
      "outputs": [],
      "source": [
        "!python server.py --model {model_file_name} --loader llamacpp --n-gpu-layers {str(n_gpu_layers)} --n_ctx {str(n_ctx)} \\\n",
        " --public-api --share"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfHZJ9CNLK7Z"
      },
      "source": [
        "# Colab-SSH 설치 및 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeAuUMaZLJ8p"
      },
      "outputs": [],
      "source": [
        "# !pip install colab-ssh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3QK__fGLSjV"
      },
      "outputs": [],
      "source": [
        "# from colab_ssh import launch_ssh\n",
        "# launch_ssh(NGROK_TOKEN, PASSWORD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_SM8GRhJ09q"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# I = np.eye(3)\n",
        "\n",
        "# while True:\n",
        "#   I = I@I"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}